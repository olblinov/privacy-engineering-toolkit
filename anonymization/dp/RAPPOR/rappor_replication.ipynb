{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAPPOR original repo (Python & R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "  \"\"\"RAPPOR encoding parameters.\n",
    "\n",
    "  These affect privacy/anonymity.  See the paper for details.\n",
    "  \"\"\"\n",
    "  def __init__(self):\n",
    "    self.num_bloombits = 16      # Number of bloom filter bits (k)\n",
    "    self.num_hashes = 2          # Number of bloom filter hashes (h)\n",
    "    self.num_cohorts = 2        # Number of cohorts (m)\n",
    "    self.prob_p = 0.50           # Probability p\n",
    "    self.prob_q = 0.75           # Probability q\n",
    "    self.prob_f = 0.50           # Probability f\n",
    "\n",
    "params = Params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "def SumBits(params, stdin, stdout):\n",
    "  csv_in = csv.reader(stdin)\n",
    "  csv_out = csv.writer(stdout)\n",
    "\n",
    "  num_cohorts = params.num_cohorts\n",
    "  num_bloombits = params.num_bloombits\n",
    "\n",
    "  sums = [[0] * num_bloombits for _ in range(num_cohorts)]\n",
    "  num_reports = [0] * num_cohorts\n",
    "\n",
    "  for i, row in enumerate(csv_in):\n",
    "    try:\n",
    "      (user_id, cohort, unused_bloom, unused_prr, irr) = row # irr is instant random response; prr is permanent random response\n",
    "    except ValueError:\n",
    "      raise RuntimeError('Error parsing row %r' % row)\n",
    "\n",
    "    if i == 0:\n",
    "      continue  # skip header\n",
    "\n",
    "    cohort = int(cohort)\n",
    "    num_reports[cohort] += 1\n",
    "    #print(\"Number of reports:\", num_reports)\n",
    "\n",
    "    if not len(irr) == params.num_bloombits:\n",
    "      raise RuntimeError(\n",
    "          \"Expected %d bits, got %r\" % (params.num_bloombits, len(irr)))\n",
    "    # sums bits in reverse\n",
    "    for i, c in enumerate(irr): # i = index; c = bit\n",
    "      bit_num = num_bloombits - i - 1  # e.g. char 0 = bit 15, char 15 = bit 0 \n",
    "      if c == '1':\n",
    "        sums[cohort][bit_num] += 1\n",
    "      else:\n",
    "        if c != '0':\n",
    "          raise RuntimeError('Invalid IRR -- digits should be 0 or 1')\n",
    "\n",
    "  for cohort in range(num_cohorts):\n",
    "    # First column is the total number of reports in the cohort.\n",
    "    row = [num_reports[cohort]] + sums[cohort] # first column is number of reports in cohort\n",
    "    csv_out.writerow(row)\n",
    "\n",
    "CSV_IN = \"\"\"\\\n",
    "user_id,cohort,bloom,prr,rappor\n",
    "5,1,dummy,dummy,0000111100001111\n",
    "5,1,dummy,dummy,0000000000111100\n",
    "3,0,dummy,dummy,0001010000111100\n",
    "\"\"\"\n",
    "\n",
    "EXPECTED_CSV_OUT = \"\"\"\\\n",
    "0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0\\r\n",
    "2,1,1,2,2,1,1,0,0,1,1,1,1,0,0,0,0\\r\n",
    "\"\"\"\n",
    "\n",
    "stdin = StringIO(CSV_IN)\n",
    "stdout = StringIO()\n",
    "\n",
    "SumBits(params, stdin, stdout)\n",
    "#print(CSV_IN)\n",
    "#print(stdout.getvalue())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notes\n",
    "- Cohorts == N of csv rows\n",
    "\n",
    "> There are 17 columns. The left-most column is the total number of reports in the cohort. The remaining 16 columns correspond to the k = 16 bits in the Bloom filter. Each column contains the number of reports with that bit set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000000010001\n",
      "0000001000000000\n",
      "0010100000000000\n",
      "0000100010000000\n",
      "0000100000001000\n",
      "0000000100010000\n",
      "apple,5,1,26,26\n",
      "banana,12,14,28,24\n",
      "carrot,4,12,25,21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import struct, hashlib\n",
    "\n",
    "def get_bloom_bits(word, cohort, num_hashes, num_bloombits):\n",
    "  \"\"\"Return an array of bits to set in the bloom filter.\n",
    "\n",
    "  In the real report, we bitwise-OR them together.  In hash candidates, we put\n",
    "  them in separate entries in the \"map\" matrix.\n",
    "  \"\"\"\n",
    "  value = struct.pack('>L', cohort) + str.encode(word)  # Encode word for each cohort. Cohort is 4 byte prefix.\n",
    "  #print(value)\n",
    "  md5 = hashlib.md5(value)\n",
    "\n",
    "  digest = md5.digest()\n",
    "  #print(word, md5.hexdigest())\n",
    "\n",
    "\n",
    "  # Each has is a byte, which means we could have up to 256 bit Bloom filters.\n",
    "  # There are 16 bytes in an MD5, in which case we can have up to 16 hash\n",
    "  # functions per Bloom filter.\n",
    "  if num_hashes > len(digest):\n",
    "    raise RuntimeError(\"Can't have more than %d hashes\" % md5)\n",
    "\n",
    "  #log('hash_input %r', value)\n",
    "  #log('Cohort %d', cohort)\n",
    "  #log('MD5 %s', md5.hexdigest())\n",
    "  #print(digest[0]) # for each of the hashes take value out of md5 created\n",
    "\n",
    "  return [digest[i] % num_bloombits for i in range(num_hashes)]\n",
    "\n",
    "def HashCandidates(params, stdin, stdout):\n",
    "  num_bloombits = params.num_bloombits\n",
    "  csv_out = csv.writer(stdout)\n",
    "\n",
    "  for line in stdin:\n",
    "    word = line.strip()\n",
    "    row = [word]\n",
    "    for cohort in range(params.num_cohorts):\n",
    "      bloom_bits = get_bloom_bits(word, cohort, params.num_hashes,\n",
    "                                         num_bloombits) # called N words * N cohorts\n",
    "      #print(bloom_bits)\n",
    "      for bit_to_set in bloom_bits:\n",
    "        # bits are indexed from 1.  Add a fixed offset for each cohort.\n",
    "        # NOTE: This detail could be omitted from the map file format, and done\n",
    "        # in R.\n",
    "        row.append(cohort * num_bloombits + (bit_to_set + 1)) # called N cohorts * N num_hashes\n",
    "      \n",
    "      #print (bloom_bits)\n",
    "      bloom = 0\n",
    "      for bit_to_set in bloom_bits:\n",
    "        bloom |= (1 << bit_to_set)\n",
    "\n",
    "      #print(bloom)\n",
    "      s = ''\n",
    "      bits = []\n",
    "      for bit_num in range(num_bloombits):\n",
    "        if bloom & (1 << bit_num):\n",
    "          bits.append('1')\n",
    "        else:\n",
    "          bits.append('0')\n",
    "      encoded_bloom = ''.join(reversed(bits))\n",
    "      print(encoded_bloom)\n",
    "\n",
    "    csv_out.writerow(row)\n",
    "\n",
    "\n",
    "STDIN = \"\"\"\\\n",
    "apple\n",
    "banana\n",
    "carrot\n",
    "\"\"\"\n",
    "\n",
    "EXPECTED_CSV_OUT = \"\"\"\\\n",
    "apple,5,1,26,26,38,34,63,62\\r\n",
    "banana,12,14,28,24,37,34,62,49\\r\n",
    "carrot,4,12,25,21,48,38,61,54\\r\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "stdin = StringIO(STDIN)\n",
    "stdout = StringIO()\n",
    "\n",
    "HashCandidates(params, stdin, stdout)\n",
    "print(stdout.getvalue())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The map file has one row per candidate. In this case, there are 60 rows: 50 for the true values and 10 for \"fake\" values, which make the candidates a superset of the true input.\n",
    "\n",
    "The left most column is the raw candidate string. Then there are 128 more columns: for m = 64 cohorts times k = 2 hash functions in the Bloom filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['__name__', '__doc__', '__package__', '__loader__', '__spec__', '__builtin__', '__builtins__', '_ih', '_oh', '_dh', 'In', 'Out', 'get_ipython', 'exit', 'quit', 'open', '_', '__', '___', '__vsc_ipynb_file__', '_i', '_ii', '_iii', '_i1', 'Params', 'params', '_i2', 'csv', 'sys', 'SumBits', '_2', '_i3', 'StringIO', 'CSV_IN', 'EXPECTED_CSV_OUT', '_i4', 'stdin', 'stdout', '_4', '_i5', '_i6', '_i7', '_i8', '_i9', '_i10', '_i11', '_i12', '_i13', '_i14', '_i15', 'struct', 'hashlib', 'get_bloom_bits', 'HashCandidates', 'STDIN', '_i16', '_i17', '_i18', '_i19', '_i20', '_i21', '_i22', '_i23', '_i24', '_i25', '_i26', '_i27', '_i28', '_i29', '_i30', '_i31', '_i32', '_i33', '_i34', '_i35', '_i36', '_i37', '_i38', '_i39', '_i40'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure LDP\n",
    "This is an attempt to replicate RAPPOR implementation by https://github.com/Samuel-Maddock/pure-LDP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "### Client\n",
    "```\n",
    "client = create_fo_client_instance(self.name, self.client_params)\n",
    "\n",
    "if not self.memory_safe:\n",
    "    for i in range(0, len(data)):\n",
    "        ldp_data.append(client.privatise(data[i]))\n",
    "```\n",
    "\n",
    "### Server\n",
    "```\n",
    "for index, item in enumerate(ldp_data):\n",
    "    server.aggregate(item)\n",
    "\n",
    "ldp_freq = server.estimate_all(domain, normalization=self.normalization)\n",
    "```\n",
    "...\n",
    "```\n",
    "def estimate_all:\n",
    "    estimates = np.array([self.estimate(x, suppress_warnings=suppress_warnings) for x in data_list])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bloombits = 128  # m - Max size is 256 bits\n",
    "num_hashes = 2  # k - Recommended to use 2 hashes\n",
    "num_of_cohorts = 8  # Max cohorts is 64\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Privatize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "[1, 0, 0, 1, 0, 0, 0, 1, 1, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([1, 0, 0, 1, 0, 0, 0, 1, 1, 0], 7)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import xxhash\n",
    "rappor_params = {\"server_params\": {\"f\": 0.64, \"m\": 10, \"k\": 2}, \"client_params\": {\"f\": 0.64, \"m\": 10}}\n",
    "\n",
    "hash_family = []\n",
    "for i in range(0, num_of_cohorts):\n",
    "    hash_family.append([\n",
    "        lambda data: xxhash.xxh64(str(data), seed=random.randint(0, sys.maxsize)).intdigest() % rappor_params['server_params']['m'] \n",
    "        for i in range(0, rappor_params['server_params']['k'])\n",
    "        ])\n",
    "\n",
    "def index_mapper(x):\n",
    "    return x - 1\n",
    "\n",
    "def privatise(data, debug=False):\n",
    "    # Create empty bloom; flip one bit per hash func (determined by k)\n",
    "\n",
    "    index = index_mapper(data)\n",
    "    cohort_num = random.randint(0, num_of_cohorts-1)\n",
    "    b = [0]*rappor_params['server_params']['m'] # empty bloom\n",
    "    hash_funcs = hash_family[cohort_num]\n",
    "    for func in hash_funcs:\n",
    "        hash_index = func(str(index)) # flip \n",
    "        b[hash_index] = 1\n",
    "    if debug: print(b) \n",
    "    \n",
    "    for i,bit in enumerate(b):\n",
    "        u = random.random()\n",
    "        if (bit == 1 and u < (1-0.5*rappor_params['client_params']['f'])) or (bit == 0 and u < 1/2*rappor_params['client_params']['f']):\n",
    "            b[i] = 1\n",
    "    if debug: print(b)\n",
    "\n",
    "\n",
    "    return b, cohort_num\n",
    "\n",
    "privatise(100, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0, 1, 0, 0, 1, 0, 1, 0, 0, 1], 5)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
